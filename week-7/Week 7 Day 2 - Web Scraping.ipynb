{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from the Internet\n",
    "\n",
    "There is a ton of good data on the internet, but it can be hard to access.  In this lesson we will learn just enough about web scraping to get in trouble.  \n",
    "\n",
    "**Important**: stay out of trouble!\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Don't break anything.  Many rapid requests to smaller sites can overload the host server.\n",
    "2. Use a published API if possible - it is more robust and usually much easier!\n",
    "3. Respect the policy published at `robots.txt` \n",
    "4. Don't spoof your UserAgent (or try to trick the server into thinking you are a person)\n",
    "5. Read the Terms of Service for the site and follow it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests\n",
    "\n",
    "`requests` is a python package that allows you to use Python to interact with the internet!  There are other packages, but I find `requests` to be much easier to use.\n",
    "\n",
    "In fact, to get the UCSD home page is a simple as\n",
    "```\n",
    "import requests\n",
    "text = requests.get(\"https://ucsd.edu\").text\n",
    "```\n",
    "But before we do that, we need to learn just a little bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status Codes\n",
    "\n",
    "When we request data from a website, the server responds with a HTTP status code.  The most common response is `200` which means things went well.  Other times you will get a different status code saying something else happened - you might be familiar with a `404` which means the page wasn't found.\n",
    "\n",
    "This great site lists http status codes: [https://httpstat.us/](https://httpstat.us/).\n",
    "\n",
    "But better yet, it has example sites that return a certain code, so you can test!  So, for example, https://httpstat.us/404 returns a `404`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the call went ok with `r.ok` which returns a boolean.\n",
    "\n",
    "After you run the code below, read up on each of the status codes at [https://httpstat.us/](https://httpstat.us/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 ok: True\n",
      "404 ok: False\n",
      "403 ok: False\n",
      "429 ok: False\n"
     ]
    }
   ],
   "source": [
    "statusCodes = [200, 404, 403, 429]\n",
    "\n",
    "for statusCode in statusCodes:\n",
    "    r = requests.get(\"https://httpstat.us/\" + str(statusCode))\n",
    "    print(str(statusCode) + \" ok: \" + str(r.ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://httpstat.us/404",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c3be1fd78071>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://httpstat.us/404\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://httpstat.us/404"
     ]
    }
   ],
   "source": [
    "# Or raise an exception when there is a not-ok status code\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robots.txt\n",
    "\n",
    "Many sites have a published policy allowing or disallowing automatic access to their site.  It uses a text file `robots.txt` and you can learn more about it [here](https://moz.com/learn/seo/robotstxt).\n",
    "\n",
    "The code below checks if the `robot.txt` file prohibits you from scraping the site.  Remember the best practices above - just because you aren't prohibited by the robots policy doesn't mean you can scrape the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "\n",
    "# This code checks the robots.txt file\n",
    "def canFetch(url):\n",
    "\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(domain + \"/robots.txt\")\n",
    "    try:\n",
    "        rp.read()\n",
    "        canFetchBool = rp.can_fetch(\"*\", url)\n",
    "    except:\n",
    "        canFetchBool = None\n",
    "    \n",
    "    return canFetchBool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://slate.com/bullpen/\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the HTML\n",
    "\n",
    "Now we can request a website!  Let's see what is on the UCSD Data Science Events page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# \">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "<meta name=\"Generator\" content=\"Drupal 8 (https://www.drupal.org)\" />\n",
      "<meta name=\"MobileOptimized\" content=\"width\" />\n",
      "<meta name=\"HandheldFriendly\" content=\"true\" />\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
      "<link rel=\"shortcut icon\" href=\"/core/misc/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\n",
      "<link rel=\"canonical\" href=\"/node/10\" />\n",
      "<link rel=\"shortlink\" href=\"/node/10\" />\n",
      "<link rel=\"revision\" href=\"/node/10\" />\n",
      "\n",
      "    <title>Events | Data Science Undergraduate Program</title>\n",
      "    <link rel=\"stylesheet\" hre\n",
      "\n",
      "\n",
      "... 14833 additional characters\n"
     ]
    }
   ],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "\n",
    "r = requests.get(url)\n",
    "    \n",
    "urlText = r.text\n",
    "\n",
    "Nchars = 1000\n",
    "print(urlText[:Nchars]) # Print the first 500 characters\n",
    "print(\"\\n\\n... \" + str(len(urlText)-Nchars) + \" additional characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "Wow, that is gross looking!  It is raw HTML, which the browser uses to make the viewable site.  To process it we can use [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "**Warning** BeautifulSoup has changed quite a bit between versions, so make sure you are looking at documentation for the version you are using (4 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(urlText, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#main-content\n",
      "#show-block-bartik-account-menu\n",
      "#hide-block-bartik-account-menu\n",
      "/user/login\n",
      "/\n",
      "/\n",
      "/\n",
      "https://drive.google.com/file/d/0B4gMdSgWOaZ_TWh5TjRGRW5RT1k/view?usp=sharing\n",
      "https://drive.google.com/file/d/0B4gMdSgWOaZ_NFN4YzB0NFdlSEk/view?usp=sharing\n",
      "https://drive.google.com/file/d/1tpMmHNTuvJHHMitddFl50FJR2mXCnpsn/view?usp=sharing\n",
      "https://drive.google.com/file/d/11bTF_C56Y_zI1dTTWiuTMPqi64aR4BFF/view?usp=sharing\n",
      "#show-block-datascienceprogram-2\n",
      "#hide-block-datascienceprogram-2\n",
      "http://dsc.ucsd.edu/node/9\n",
      "https://dsc.ucsd.edu/node/13\n",
      "http://dsc.ucsd.edu/node/10\n",
      "#show-block-datasciencemajor\n",
      "#hide-block-datasciencemajor\n",
      "http://dsc.ucsd.edu/node/3\n",
      "http://dsc.ucsd.edu/node/7\n",
      "#show-block-datascienceminor\n",
      "#hide-block-datascienceminor\n",
      "http://dsc.ucsd.edu/node/11\n",
      "#show-block-datasciencecourses\n",
      "#hide-block-datasciencecourses\n",
      "https://dsc.ucsd.edu/node/15\n",
      "http://dsc.ucsd.edu/node/4\n",
      "https://dsc.ucsd.edu/node/16\n",
      "#show-block-resources\n",
      "#hide-block-resources\n",
      "http://dsc.ucsd.edu/node/2\n",
      "https://www.drupal.org\n",
      "#show-block-bartik-footer\n",
      "#hide-block-bartik-footer\n",
      "/contact\n"
     ]
    }
   ],
   "source": [
    "# Grab all the links\n",
    "for link in soup.find_all('a'):\n",
    "    if link.has_attr('href'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-648180fa3859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Show the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the text\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Events | Data Science Undergraduate Program'\n",
      "'Skip to main content'\n",
      "'User account menu'\n",
      "'Show — User account menu'\n",
      "'Hide — User account menu'\n",
      "'Log in'\n",
      "'Data Science Undergraduate Program'\n",
      "'Data Science Undergraduate Program'\n",
      "'Breadcrumb'\n",
      "'Home'\n",
      "'Events'\n",
      "'Data Science Events'\n",
      "'Data Science Town Hall Meeting'\n",
      "': 7 pm, Tuesday, May 30, 2017; Ledden Auditoriun'\n",
      "'Data Science Town Hall Meeting'\n",
      "': 2 pm, Tuesday, September 26, 2017; CSE 1202'\n",
      "'Data Science Group Advising Session'\n",
      "': 3 pm, Tuesday, February 13th, CSE  1202'\n",
      "'In this session, students will be advised about enrolling in data science classes for Spring 2018'\n",
      "'End of the Year Town Hall Meeting'\n",
      "': 5-7 pm, Monday, April 30th, 2018;  Atkinson Hall Auditorium'\n",
      "'Course planning for 2018-19 academic year will be discussed during the town hall meeting.'\n",
      "'Interested in learning more about events happening on campus related to Data Science? Subscribe to our newsletter below!'\n",
      "'<!--/*--><![CDATA[/* ><!--*/\\n#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }\\n\\t/* Add your own MailChimp form style overrides in your site stylesheet or in this style block.\\n\\t   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */\\n\\n/*--><!]]>*/'\n",
      "'Subscribe to our mailing list'\n",
      "'Data Science Program'\n",
      "'Show — Data Science Program'\n",
      "'Hide — Data Science Program'\n",
      "'Advising'\n",
      "'Course Guidance'\n",
      "'Events'\n",
      "'Data Science Major'\n",
      "'Show — Data Science Major'\n",
      "'Hide — Data Science Major'\n",
      "'Entry into the major'\n",
      "'Major Requirements'\n",
      "'Data Science Minor'\n",
      "'Show — Data Science Minor'\n",
      "'Hide — Data Science Minor'\n",
      "'Minor Requirements'\n",
      "'Data Science Courses'\n",
      "'Show — Data Science Courses'\n",
      "'Hide — Data Science Courses'\n",
      "'2018-19 Course Offerings'\n",
      "'Lower-division courses'\n",
      "'Upper-division courses'\n",
      "'Data Science Resources'\n",
      "'Show — Data Science Resources'\n",
      "'Hide — Data Science Resources'\n",
      "'What is Data Science?'\n",
      "'Search'\n",
      "'Search'\n",
      "'Powered by'\n",
      "'Drupal'\n",
      "'Footer menu'\n",
      "'Show — Footer menu'\n",
      "'Hide — Footer menu'\n",
      "'Contact'\n"
     ]
    }
   ],
   "source": [
    "# That text had too much white space.  Let's try\n",
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "From here you can do a number of different things!\n",
    "\n",
    "* Scrape individual elements from a site ([example](https://codeburst.io/web-scraping-101-with-python-beautiful-soup-bb617be1f486))\n",
    "* Pull text down and use NLP from last week (like sentiment analysis)\n",
    "* Monitor a site daily for changes.\n",
    "* Use the text to create your own search engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
